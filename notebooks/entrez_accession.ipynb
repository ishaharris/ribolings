{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f733e508",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m virus = \u001b[33m'\u001b[39m\u001b[33mArena\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Define the folder and file name\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ribolings/lib/python3.11/site-packages/pandas/__init__.py:14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _dependency \u001b[38;5;129;01min\u001b[39;00m _hard_dependencies:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_dependency\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _e:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m     16\u001b[39m         _missing_dependencies.append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_dependency\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_e\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ribolings/lib/python3.11/site-packages/numpy/__init__.py:127\u001b[39m\n\u001b[32m    124\u001b[39m     sys.stderr.write(\u001b[33m'\u001b[39m\u001b[33mRunning from numpy source directory.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    126\u001b[39m     \u001b[38;5;66;03m# Allow distributors to run custom init code before importing numpy.core\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _distributor_init\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    130\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__config__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show \u001b[38;5;28;01mas\u001b[39;00m show_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ribolings/lib/python3.11/site-packages/numpy/_distributor_init.py:14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmkl\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     __mkl_version__ = \u001b[33m\"\u001b[39m\u001b[38;5;132;01m{MajorVersion}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{MinorVersion}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{UpdateVersion}\u001b[39;00m\u001b[33m\"\u001b[39m.format(**\u001b[43mmkl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "virus = 'Arena'\n",
    "\n",
    "# Define the folder and file name\n",
    "folder = Path(\"/Users/ishaharris/Projects/ribolings/data/virus/codes/rna\")  # adjust to your folder\n",
    "file = f\"{virus}_codes.csv\"\n",
    "\n",
    "# Build the full path using /\n",
    "file_path = folder / file\n",
    "\n",
    "# Read CSV into a pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Show first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b223dc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users/ishaharris/Projects/ribolings/data/virus/codes/rna/Arena_codes.csv\n"
     ]
    }
   ],
   "source": [
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d8921d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === Retrieve GenBank CDS FASTAs by accession (hard-coded API key + robust skip) ===\n",
    "from __future__ import annotations\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from Bio import Entrez\n",
    "\n",
    "# --------------- CONFIG (edit these) ---------------\n",
    "# REQUIRED: set your real email and API key (hard-coded here)\n",
    "ENTREZ_EMAIL   = \"inah2@cam.ac.uk\"\n",
    "ENTREZ_API_KEY = None\n",
    "#ENTREZ_API_KEY = \"fe0657db7c3e1518b265167f026e11288a07\"   # <-- put your key here\n",
    "\n",
    "# Input directory with CSVs like \"Sedereo_codes.csv\" containing a column named \"Codes\"\n",
    "INPUT_DIR  = Path(\"/Users/ishaharris/Projects/ribolings/data/virus/codes/dna\")\n",
    "\n",
    "# Output directory for combined FASTAs + failed accession logs\n",
    "OUTPUT_DIR = Path(\"/Users/ishaharris/Projects/ribolings/data/virus/dna_cds_fasta/\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set to a list (e.g., [\"Sedereo\",\"Birna\"]) to restrict; or leave as None to auto-discover all *_codes.csv\n",
    "VIRUS_FAMILIES: Optional[List[str]] = None\n",
    "\n",
    "# Gentle rate-limiting between NCBI calls (seconds). With API key, you can go to ~0.1s safely.\n",
    "SLEEP = 0.2\n",
    "# ------------- /CONFIG ----------------\n",
    "\n",
    "# Apply Entrez credentials\n",
    "Entrez.email = ENTREZ_EMAIL\n",
    "Entrez.api_key = ENTREZ_API_KEY\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def fasta_record_count(path: Path) -> int:\n",
    "    \"\"\"Count records in an existing FASTA (lines starting with '>'). Returns 0 if file missing or empty.\"\"\"\n",
    "    if not path.exists() or path.stat().st_size == 0:\n",
    "        return 0\n",
    "    try:\n",
    "        cnt = 0\n",
    "        with path.open(\"r\") as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\">\"):\n",
    "                    cnt += 1\n",
    "        return cnt\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def entrez_search_nuccore(accession: str) -> List[str]:\n",
    "    \"\"\"Return a list of nuccore IDs for the accession (may be empty). Uses XML (default) so Entrez.read works.\"\"\"\n",
    "    query = f\"{accession}[accession]\"\n",
    "    with Entrez.esearch(db=\"nuccore\", term=query) as handle:   # XML by default\n",
    "        data = Entrez.read(handle)                              # parse XML\n",
    "    return data.get(\"IdList\", []) or []\n",
    "\n",
    "def entrez_fetch_fasta_cds_na(nuccore_id: str) -> str:\n",
    "    \"\"\"Fetch FASTA (CDS nucleotides) text for a single nuccore id.\"\"\"\n",
    "    with Entrez.efetch(db=\"nuccore\", id=nuccore_id, rettype=\"fasta_cds_na\", retmode=\"text\") as handle:\n",
    "        return handle.read()\n",
    "\n",
    "def retrieve_cds_sequences(accession: str) -> Optional[List[str]]:\n",
    "    \"\"\"\n",
    "    Search by accession; fetch rettype=fasta_cds_na for the first hit.\n",
    "    Returns list of 'header\\\\nsequence' chunks (without leading '>'), or None if not found.\n",
    "    \"\"\"\n",
    "    ids = entrez_search_nuccore(accession)\n",
    "    time.sleep(SLEEP)\n",
    "    if not ids:\n",
    "        return None\n",
    "    blob = entrez_fetch_fasta_cds_na(ids[0])\n",
    "    time.sleep(SLEEP)\n",
    "    # Split on '>' and remove internal newlines (parity with R's gsub)\n",
    "    parts = [p.replace(\"\\n\", \"\") for p in blob.split(\">\") if p.strip()]\n",
    "    return parts or None\n",
    "\n",
    "def process_fasta_chunks(fasta_chunks: List[str]) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Each chunk looks like: \"<header fields...>[gbkey=CDS]<sequence>\"\n",
    "    We split on \"[gbkey=CDS]\" and reconstruct:\n",
    "      metadata = \"<header fields...>[gbkey=CDS]\"\n",
    "      nucleotides = \"<sequence>\"\n",
    "    \"\"\"\n",
    "    metas, seqs = [], []\n",
    "    for chunk in fasta_chunks:\n",
    "        parts = chunk.split(\"[gbkey=CDS]\")\n",
    "        if len(parts) < 2:\n",
    "            continue\n",
    "        meta = (parts[0] + \"[gbkey=CDS]\").strip()\n",
    "        seq  = parts[1].strip().replace(\" \", \"\")\n",
    "        if seq:\n",
    "            metas.append(meta)\n",
    "            seqs.append(seq)\n",
    "    return metas, seqs\n",
    "\n",
    "def write_fasta(headers: List[str], seqs: List[str], out_path: Path, line_width: int = 80) -> None:\n",
    "    \"\"\"Write a multi-record FASTA; each header goes after '>'.\"\"\"\n",
    "    assert len(headers) == len(seqs)\n",
    "    def wrap(s: str, w: int) -> List[str]:\n",
    "        return [s[i:i+w] for i in range(0, len(s), w)]\n",
    "    with out_path.open(\"w\") as f:\n",
    "        for h, s in zip(headers, seqs):\n",
    "            f.write(f\">{h}\\n\")\n",
    "            for line in wrap(s.upper(), line_width):\n",
    "                f.write(line + \"\\n\")\n",
    "\n",
    "def discover_families(input_dir: Path) -> List[str]:\n",
    "    \"\"\"Find all CSVs matching *_codes.csv and return family names.\"\"\"\n",
    "    fams = []\n",
    "    for p in input_dir.glob(\"*_codes.csv\"):\n",
    "        name = p.stem\n",
    "        if name.endswith(\"_codes\"):\n",
    "            fams.append(name[:-6])  # strip suffix \"_codes\"\n",
    "    return sorted(set(fams))\n",
    "\n",
    "def process_family(family: str) -> None:\n",
    "    print(f\"\\n=== {family} ===\")\n",
    "\n",
    "    # Robust skip: only skip if FASTA exists AND has at least 1 record\n",
    "    out_fa = OUTPUT_DIR / f\"{family}.fasta\"\n",
    "    n_existing = fasta_record_count(out_fa)\n",
    "    if n_existing > 0:\n",
    "        print(f\"[SKIP] {family} already processed: {out_fa} ({n_existing} records).\")\n",
    "        return\n",
    "    elif out_fa.exists():\n",
    "        print(f\"[INFO] {out_fa} exists but contains 0 records — reprocessing.\")\n",
    "\n",
    "    csv_path = INPUT_DIR / f\"{family}_codes.csv\"\n",
    "    if not csv_path.exists():\n",
    "        print(f\"[WARN] Missing: {csv_path}\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Be tolerant to slight column naming differences (e.g. 'codes')\n",
    "    col = None\n",
    "    for cand in [\"Codes\", \"codes\", \"ACCESSION\", \"accession\", \"Accession\"]:\n",
    "        if cand in df.columns:\n",
    "            col = cand\n",
    "            break\n",
    "    if col is None:\n",
    "        raise ValueError(f\"No accession column found in {csv_path}. Columns present: {list(df.columns)}\")\n",
    "\n",
    "    accessions = (\n",
    "        df[col]\n",
    "        .dropna()\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .replace(\"\", pd.NA)\n",
    "        .dropna()\n",
    "        .unique()\n",
    "    )\n",
    "    print(f\"Found {len(accessions)} unique accessions in {csv_path.name}\")\n",
    "\n",
    "    if len(accessions) == 0:\n",
    "        print(\"[WARN] No accessions after cleaning; nothing to do.\")\n",
    "        return\n",
    "\n",
    "    all_chunks: List[str] = []\n",
    "    failed: List[str] = []\n",
    "\n",
    "    for i, acc in enumerate(accessions, 1):\n",
    "        try:\n",
    "            chunks = retrieve_cds_sequences(acc)\n",
    "            if chunks:\n",
    "                all_chunks.extend(chunks)\n",
    "            else:\n",
    "                failed.append(acc)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERR] {acc}: {e}\")\n",
    "            failed.append(acc)\n",
    "\n",
    "        if i % 25 == 0 or i == len(accessions):\n",
    "            print(f\"  …processed {i}/{len(accessions)}\")\n",
    "\n",
    "    if not all_chunks:\n",
    "        print(\"[INFO] No CDS records returned for this family.\")\n",
    "    else:\n",
    "        metas, seqs = process_fasta_chunks(all_chunks)\n",
    "        print(f\"Writing {len(seqs)} CDS entries to FASTA\")\n",
    "        write_fasta(metas, seqs, out_fa)\n",
    "        final_cnt = fasta_record_count(out_fa)\n",
    "        print(f\"[OK] Wrote {out_fa} with {final_cnt} records.\")\n",
    "\n",
    "    # Log failures (if any)\n",
    "    fail_path = OUTPUT_DIR / f\"{family}_failed_accessions.txt\"\n",
    "    if failed:\n",
    "        with fail_path.open(\"w\") as f:\n",
    "            f.write(\"\\n\".join(failed) + \"\\n\")\n",
    "        print(f\"[WARN] {len(failed)} accessions failed. Logged to {fail_path}\")\n",
    "    else:\n",
    "        # If a previous fail file exists but now there are none, remove to keep tidy\n",
    "        if fail_path.exists():\n",
    "            try:\n",
    "                fail_path.unlink()\n",
    "            except Exception:\n",
    "                pass\n",
    "        print(\"[OK] No failed accessions.\")\n",
    "\n",
    "# ----------------- Run in Jupyter -----------------\n",
    "if VIRUS_FAMILIES is None:\n",
    "    families = discover_families(INPUT_DIR)\n",
    "    print(f\"Discovered {len(families)} families from {INPUT_DIR}:\", families)\n",
    "else:\n",
    "    families = VIRUS_FAMILIES\n",
    "    print(\"Using specified families:\", families)\n",
    "\n",
    "for fam in families:\n",
    "    process_family(fam)\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c4ac40",
   "metadata": {},
   "source": [
    "tried to fix the metadata, also check for api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e095f725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] No API key set; using default quota (~3 req/s).\n",
      "Discovered 23 families from /Users/ishaharris/Projects/ribolings/data/virus/codes/rna: ['Arena', 'Arteri', 'Astro', 'Birna', 'Borna', 'Calici', 'Corona', 'DNA', 'Filo', 'Flavi', 'Hanta', 'Hepe', 'Kolmio', 'Noda', 'Orthomyxo', 'Paramyxo', 'Peribunya', 'Picorna', 'Pneumo', 'Rhabdo', 'Sedereo', 'Spinareo', 'Tobani']\n",
      "\n",
      "=== Arena ===\n",
      "Found 137 unique accessions in Arena_codes.csv\n",
      "  …processed 25/137 (~0.63 acc/s overall)\n",
      "  …processed 50/137 (~0.65 acc/s overall)\n",
      "  …processed 75/137 (~0.63 acc/s overall)\n",
      "  …processed 100/137 (~0.64 acc/s overall)\n",
      "  …processed 125/137 (~0.64 acc/s overall)\n",
      "  …processed 137/137 (~0.64 acc/s overall)\n",
      "[STATS] Arena: 137 accessions in 213.2s ≈ 0.64 acc/s\n",
      "Writing 249 CDS entries to FASTA\n",
      "[OK] Wrote /Users/ishaharris/Projects/ribolings/data/virus/cds_fasta/Arena.fasta with 249 records.\n",
      "[OK] All headers contained [gbkey=CDS].\n",
      "[WARN] 11 sequences contain chars outside A/C/G/T/N. Logged to /Users/ishaharris/Projects/ribolings/data/virus/cds_fasta/Arena_nonACGTN_sequences.txt\n",
      "[OK] No failed accessions.\n",
      "\n",
      "=== Arteri ===\n",
      "Found 23 unique accessions in Arteri_codes.csv\n",
      "  …processed 23/23 (~0.64 acc/s overall)\n",
      "[STATS] Arteri: 23 accessions in 36.0s ≈ 0.64 acc/s\n",
      "Writing 254 CDS entries to FASTA\n",
      "[OK] Wrote /Users/ishaharris/Projects/ribolings/data/virus/cds_fasta/Arteri.fasta with 254 records.\n",
      "[OK] All headers contained [gbkey=CDS].\n",
      "[WARN] 6 sequences contain chars outside A/C/G/T/N. Logged to /Users/ishaharris/Projects/ribolings/data/virus/cds_fasta/Arteri_nonACGTN_sequences.txt\n",
      "[OK] No failed accessions.\n",
      "\n",
      "=== Astro ===\n",
      "Found 22 unique accessions in Astro_codes.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 260\u001b[39m\n\u001b[32m    257\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUsing specified families:\u001b[39m\u001b[33m\"\u001b[39m, families)\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fam \u001b[38;5;129;01min\u001b[39;00m families:\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m     \u001b[43mprocess_family\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDone.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 182\u001b[39m, in \u001b[36mprocess_family\u001b[39m\u001b[34m(family)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m acc \u001b[38;5;129;01min\u001b[39;00m accessions:\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m         recs = \u001b[43mretrieve_cds_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43macc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m recs:\n\u001b[32m    184\u001b[39m             all_records.extend(recs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36mretrieve_cds_records\u001b[39m\u001b[34m(accession)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[33;03mSearch by accession; fetch rettype=fasta_cds_na for the first hit.\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[33;03mReturn a list of raw FASTA records (each starting with '>'), or None if none.\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     81\u001b[39m ids = entrez_search_nuccore(accession)\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m time.sleep(SLEEP)\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ids:\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === Retrieve GenBank CDS FASTAs by accession (robust FASTA parsing + checks) ===\n",
    "from __future__ import annotations\n",
    "\n",
    "import time, re\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from Bio import Entrez\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "# --------------- CONFIG (edit these) ---------------\n",
    "ENTREZ_EMAIL   = \"inah2@cam.ac.uk\"\n",
    "ENTREZ_API_KEY = None  # or paste your key string here e.g. \"fe0657db7c3e1518b265167f026e11288a07\"\n",
    "\n",
    "# Input CSVs like \"Sedereo_codes.csv\" with a column of accessions (e.g. \"Codes\")\n",
    "INPUT_DIR  = Path(\"/Users/ishaharris/Projects/ribolings/data/virus/codes/rna\")\n",
    "\n",
    "# Output folder for combined FASTAs + logs\n",
    "OUTPUT_DIR = Path(\"/Users/ishaharris/Projects/ribolings/data/virus/cds_fasta/\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Families to process (None = auto-discover all *_codes.csv in INPUT_DIR)\n",
    "VIRUS_FAMILIES: Optional[List[str]] = None\n",
    "\n",
    "# Throttle between requests (0.2s is fine without a key; 0.1–0.2s with a key)\n",
    "SLEEP = 0.2\n",
    "\n",
    "# Progress cadence and skip rule\n",
    "PROGRESS_EVERY = 25\n",
    "MIN_RECORDS_TO_SKIP = 1\n",
    "# ------------- /CONFIG ----------------\n",
    "\n",
    "# Apply Entrez credentials + validate API key\n",
    "Entrez.email = ENTREZ_EMAIL\n",
    "_key = (ENTREZ_API_KEY or \"\")\n",
    "_key = _key.strip() if isinstance(_key, str) else \"\"\n",
    "if _key and re.fullmatch(r\"[A-Za-z0-9\\-]{20,80}\", _key):\n",
    "    Entrez.api_key = _key\n",
    "    print(f\"[INFO] Using NCBI API key (len={len(_key)}).\")\n",
    "else:\n",
    "    Entrez.api_key = None\n",
    "    if _key:\n",
    "        print(f\"[WARN] API key looks invalid (len={len(_key)}). Not using it.\")\n",
    "    else:\n",
    "        print(\"[INFO] No API key set; using default quota (~3 req/s).\")\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def fasta_record_count(path: Path) -> int:\n",
    "    \"\"\"Count FASTA records (lines starting with '>').\"\"\"\n",
    "    try:\n",
    "        if not path.exists() or path.stat().st_size == 0:\n",
    "            return 0\n",
    "        return sum(1 for line in path.open() if line.startswith(\">\"))\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def entrez_search_nuccore(accession: str) -> List[str]:\n",
    "    \"\"\"Search nuccore by accession; return a list of IDs (may be empty).\"\"\"\n",
    "    query = f\"{accession}[accession]\"\n",
    "    with Entrez.esearch(db=\"nuccore\", term=query) as handle:  # XML default\n",
    "        data = Entrez.read(handle)\n",
    "    return data.get(\"IdList\", []) or []\n",
    "\n",
    "def efetch_cds(nuccore_id: str) -> Optional[str]:\n",
    "    \"\"\"Fetch CDS nucleotide FASTA text for a single nuccore id; return None if not available.\"\"\"\n",
    "    try:\n",
    "        with Entrez.efetch(db=\"nuccore\", id=nuccore_id, rettype=\"fasta_cds_na\", retmode=\"text\") as h:\n",
    "            return h.read()\n",
    "    except HTTPError as e:\n",
    "        # 400 typically means no CDS view for this record (e.g., wrong type / no CDS features)\n",
    "        if e.code == 400:\n",
    "            return None\n",
    "        raise\n",
    "\n",
    "def retrieve_cds_records(accession: str) -> Optional[List[str]]:\n",
    "    \"\"\"\n",
    "    Search by accession; fetch rettype=fasta_cds_na for the first hit.\n",
    "    Return a list of raw FASTA records (each starting with '>'), or None if none.\n",
    "    \"\"\"\n",
    "    ids = entrez_search_nuccore(accession)\n",
    "    time.sleep(SLEEP)\n",
    "    if not ids:\n",
    "        return None\n",
    "\n",
    "    blob = efetch_cds(ids[0])\n",
    "    time.sleep(SLEEP)\n",
    "    if not blob:\n",
    "        return None\n",
    "\n",
    "    # Split into individual FASTA records; keep leading '>'\n",
    "    records = [\">\" + chunk for chunk in blob.split(\"\\n>\") if chunk.strip()]\n",
    "    return records or None\n",
    "\n",
    "def parse_fasta_records(fasta_records: List[str]) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Robust FASTA parsing:\n",
    "      - header = first line after '>'\n",
    "      - sequence = concat of remaining lines with whitespace removed\n",
    "    Returns (headers, sequences).\n",
    "    \"\"\"\n",
    "    headers, seqs = [], []\n",
    "    for rec in fasta_records:\n",
    "        rec = rec.lstrip()\n",
    "        if not rec.startswith(\">\"):\n",
    "            continue\n",
    "        lines = rec.splitlines()\n",
    "        header = lines[0][1:].strip()  # drop '>'\n",
    "        seq = \"\".join(lines[1:]).upper()\n",
    "        # remove whitespace and carriage returns\n",
    "        seq = re.sub(r\"\\s+\", \"\", seq)\n",
    "        if seq:\n",
    "            headers.append(header)\n",
    "            seqs.append(seq)\n",
    "    return headers, seqs\n",
    "\n",
    "def write_fasta(headers: List[str], seqs: List[str], out_path: Path, line_width: int = 80) -> None:\n",
    "    \"\"\"Write a multi-record FASTA; each header goes after '>'.\"\"\"\n",
    "    assert len(headers) == len(seqs)\n",
    "    def wrap(s: str, w: int) -> List[str]:\n",
    "        return [s[i:i+w] for i in range(0, len(s), w)]\n",
    "    with out_path.open(\"w\") as f:\n",
    "        for h, s in zip(headers, seqs):\n",
    "            f.write(f\">{h}\\n\")\n",
    "            for line in wrap(s, line_width):\n",
    "                f.write(line + \"\\n\")\n",
    "\n",
    "def discover_families(input_dir: Path) -> List[str]:\n",
    "    \"\"\"Auto-discover all families with *_codes.csv files.\"\"\"\n",
    "    fams = []\n",
    "    for p in input_dir.glob(\"*_codes.csv\"):\n",
    "        name = p.stem\n",
    "        if name.endswith(\"_codes\"):\n",
    "            fams.append(name[:-6])\n",
    "    return sorted(set(fams))\n",
    "\n",
    "def process_family(family: str) -> None:\n",
    "    print(f\"\\n=== {family} ===\")\n",
    "    out_fa = OUTPUT_DIR / f\"{family}.fasta\"\n",
    "\n",
    "    # Skip if we already have a non-empty FASTA\n",
    "    n_existing = fasta_record_count(out_fa)\n",
    "    if n_existing >= MIN_RECORDS_TO_SKIP:\n",
    "        print(f\"[SKIP] {family} already processed: {out_fa} ({n_existing} records).\")\n",
    "        return\n",
    "    elif out_fa.exists():\n",
    "        print(f\"[INFO] {out_fa} exists but has {n_existing} records — reprocessing.\")\n",
    "\n",
    "    csv_path = INPUT_DIR / f\"{family}_codes.csv\"\n",
    "    if not csv_path.exists():\n",
    "        print(f\"[WARN] Missing: {csv_path}\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    col = next((c for c in [\"Codes\",\"codes\",\"ACCESSION\",\"accession\",\"Accession\"] if c in df.columns), None)\n",
    "    if col is None:\n",
    "        raise ValueError(f\"No accession column found in {csv_path}. Columns present: {list(df.columns)}\")\n",
    "\n",
    "    accessions = (\n",
    "        df[col]\n",
    "        .dropna()\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .replace(\"\", pd.NA)\n",
    "        .dropna()\n",
    "        .unique()\n",
    "    )\n",
    "    total = len(accessions)\n",
    "    print(f\"Found {total} unique accessions in {csv_path.name}\")\n",
    "    if total == 0:\n",
    "        print(\"[WARN] No accessions after cleaning; nothing to do.\")\n",
    "        return\n",
    "\n",
    "    # Fetch + parse\n",
    "    all_records: List[str] = []\n",
    "    failed: List[str] = []\n",
    "    t0 = time.time()\n",
    "    processed = 0\n",
    "\n",
    "    for acc in accessions:\n",
    "        try:\n",
    "            recs = retrieve_cds_records(acc)\n",
    "            if recs:\n",
    "                all_records.extend(recs)\n",
    "            else:\n",
    "                failed.append(acc)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERR] {acc}: {e}\")\n",
    "            failed.append(acc)\n",
    "\n",
    "        processed += 1\n",
    "        if (processed % PROGRESS_EVERY == 0) or (processed == total):\n",
    "            rate_total = processed / max(1e-9, (time.time() - t0))\n",
    "            print(f\"  …processed {processed}/{total} (~{rate_total:.2f} acc/s overall)\")\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    if elapsed > 0:\n",
    "        print(f\"[STATS] {family}: {processed} accessions in {elapsed:.1f}s ≈ {processed/elapsed:.2f} acc/s\")\n",
    "\n",
    "    if not all_records:\n",
    "        print(\"[INFO] No CDS records returned for this family.\")\n",
    "        # still log failures if any\n",
    "        if failed:\n",
    "            fail_path = OUTPUT_DIR / f\"{family}_failed_accessions.txt\"\n",
    "            with fail_path.open(\"w\") as f:\n",
    "                f.write(\"\\n\".join(failed) + \"\\n\")\n",
    "            print(f\"[WARN] {len(failed)} accessions failed. Logged to {fail_path}\")\n",
    "        return\n",
    "\n",
    "    # Parse into headers + sequences\n",
    "    metas, seqs = parse_fasta_records(all_records)\n",
    "    print(f\"Writing {len(seqs)} CDS entries to FASTA\")\n",
    "    write_fasta(metas, seqs, out_fa)\n",
    "    print(f\"[OK] Wrote {out_fa} with {fasta_record_count(out_fa)} records.\")\n",
    "\n",
    "    # ---- Sanity checks ----\n",
    "    # 1) Non-standard metadata: headers missing [gbkey=CDS]\n",
    "    nonstandard = [m for m in metas if \"[gbkey=CDS]\" not in m]\n",
    "    if nonstandard:\n",
    "        warn_path = OUTPUT_DIR / f\"{family}_nonstandard_headers.txt\"\n",
    "        with warn_path.open(\"w\") as f:\n",
    "            f.write(\"\\n\".join(nonstandard) + \"\\n\")\n",
    "        print(f\"[WARN] {len(nonstandard)} headers missing [gbkey=CDS]. Logged to {warn_path}\")\n",
    "    else:\n",
    "        print(\"[OK] All headers contained [gbkey=CDS].\")\n",
    "\n",
    "    # 2) Non-ACGTN characters in sequences (e.g., ambiguity beyond N)\n",
    "    bad_seq_idx = [i for i, s in enumerate(seqs) if re.search(r\"[^ACGTN]\", s)]\n",
    "    if bad_seq_idx:\n",
    "        bad_path = OUTPUT_DIR / f\"{family}_nonACGTN_sequences.txt\"\n",
    "        with bad_path.open(\"w\") as f:\n",
    "            for i in bad_seq_idx:\n",
    "                f.write(f\">{metas[i]}\\n{seqs[i]}\\n\")\n",
    "        print(f\"[WARN] {len(bad_seq_idx)} sequences contain chars outside A/C/G/T/N. Logged to {bad_path}\")\n",
    "    else:\n",
    "        print(\"[OK] All sequences A/C/G/T/N only.\")\n",
    "\n",
    "    # Log failures (if any)\n",
    "    if failed:\n",
    "        fail_path = OUTPUT_DIR / f\"{family}_failed_accessions.txt\"\n",
    "        with fail_path.open(\"w\") as f:\n",
    "            f.write(\"\\n\".join(failed) + \"\\n\")\n",
    "        print(f\"[WARN] {len(failed)} accessions with no CDS returned. Logged to {fail_path}\")\n",
    "    else:\n",
    "        cleanup = OUTPUT_DIR / f\"{family}_failed_accessions.txt\"\n",
    "        if cleanup.exists():\n",
    "            try: cleanup.unlink()\n",
    "            except Exception: pass\n",
    "        print(\"[OK] No failed accessions.\")\n",
    "\n",
    "# ----------------- Run -----------------\n",
    "if VIRUS_FAMILIES is None:\n",
    "    families = discover_families(INPUT_DIR)\n",
    "    print(f\"Discovered {len(families)} families from {INPUT_DIR}:\", families)\n",
    "else:\n",
    "    families = VIRUS_FAMILIES\n",
    "    print(\"Using specified families:\", families)\n",
    "\n",
    "for fam in families:\n",
    "    process_family(fam)\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8be01e0",
   "metadata": {},
   "source": [
    "# THIS ONE WORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d797cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] No API key set; using default quota (~3 req/s).\n",
      "Discovered 10 families from /Users/ishaharris/Projects/ribolings/data/virus/codes/dna: ['Adeno', 'Anello', 'Asfar', 'Circo', 'Herpes', 'Irido', 'Papilloma', 'Parvo', 'Polyomar', 'Pox']\n",
      "\n",
      "=== TEST Arena (first 5 accessions) ===\n",
      "[WARN] Missing: /Users/ishaharris/Projects/ribolings/data/virus/codes/dna/Arena_codes.csv\n",
      "\n",
      "=== Adeno ===\n",
      "Found 10 unique accessions in Adeno_codes.csv\n",
      "  …processed 10/10 (~0.65 acc/s overall)\n",
      "[STATS] Adeno: 10 accessions in 15.5s ≈ 0.65 acc/s\n",
      "Writing 296 CDS entries to FASTA\n",
      "[OK] Wrote /Users/ishaharris/Projects/ribolings/data/virus/dna_cds_fasta/Adeno.fasta with 296 records.\n",
      "[OK] All headers contained [gbkey=CDS].\n",
      "[OK] All sequences A/C/G/T/N only.\n",
      "[OK] No failed accessions.\n",
      "\n",
      "=== Anello ===\n",
      "Found 1 unique accessions in Anello_codes.csv\n",
      "  …processed 1/1 (~0.81 acc/s overall)\n",
      "[STATS] Anello: 1 accessions in 1.2s ≈ 0.81 acc/s\n",
      "Writing 3 CDS entries to FASTA\n",
      "[OK] Wrote /Users/ishaharris/Projects/ribolings/data/virus/dna_cds_fasta/Anello.fasta with 3 records.\n",
      "[OK] All headers contained [gbkey=CDS].\n",
      "[OK] All sequences A/C/G/T/N only.\n",
      "[OK] No failed accessions.\n",
      "\n",
      "=== Asfar ===\n",
      "Found 1 unique accessions in Asfar_codes.csv\n",
      "  …processed 1/1 (~0.62 acc/s overall)\n",
      "[STATS] Asfar: 1 accessions in 1.6s ≈ 0.62 acc/s\n",
      "Writing 152 CDS entries to FASTA\n",
      "[OK] Wrote /Users/ishaharris/Projects/ribolings/data/virus/dna_cds_fasta/Asfar.fasta with 152 records.\n",
      "[OK] All headers contained [gbkey=CDS].\n",
      "[OK] All sequences A/C/G/T/N only.\n",
      "[OK] No failed accessions.\n",
      "\n",
      "=== Circo ===\n",
      "Found 5 unique accessions in Circo_codes.csv\n",
      "  …processed 5/5 (~0.70 acc/s overall)\n",
      "[STATS] Circo: 5 accessions in 7.1s ≈ 0.70 acc/s\n",
      "Writing 22 CDS entries to FASTA\n",
      "[OK] Wrote /Users/ishaharris/Projects/ribolings/data/virus/dna_cds_fasta/Circo.fasta with 22 records.\n",
      "[OK] All headers contained [gbkey=CDS].\n",
      "[WARN] 1 sequences contain chars outside A/C/G/T/N. Logged to /Users/ishaharris/Projects/ribolings/data/virus/dna_cds_fasta/Circo_nonACGTN_sequences.txt\n",
      "[OK] No failed accessions.\n",
      "\n",
      "=== Herpes ===\n",
      "Found 16 unique accessions in Herpes_codes.csv\n",
      "  …processed 16/16 (~0.59 acc/s overall)\n",
      "[STATS] Herpes: 16 accessions in 27.0s ≈ 0.59 acc/s\n",
      "Writing 1632 CDS entries to FASTA\n",
      "[OK] Wrote /Users/ishaharris/Projects/ribolings/data/virus/dna_cds_fasta/Herpes.fasta with 1632 records.\n",
      "[OK] All headers contained [gbkey=CDS].\n",
      "[WARN] 1 sequences contain chars outside A/C/G/T/N. Logged to /Users/ishaharris/Projects/ribolings/data/virus/dna_cds_fasta/Herpes_nonACGTN_sequences.txt\n",
      "[OK] No failed accessions.\n",
      "\n",
      "=== Irido ===\n",
      "Found 3 unique accessions in Irido_codes.csv\n",
      "  …processed 3/3 (~0.62 acc/s overall)\n",
      "[STATS] Irido: 3 accessions in 4.8s ≈ 0.62 acc/s\n",
      "Writing 463 CDS entries to FASTA\n",
      "[OK] Wrote /Users/ishaharris/Projects/ribolings/data/virus/dna_cds_fasta/Irido.fasta with 463 records.\n",
      "[OK] All headers contained [gbkey=CDS].\n",
      "[OK] All sequences A/C/G/T/N only.\n",
      "[OK] No failed accessions.\n",
      "\n",
      "=== Papilloma ===\n",
      "Found 12 unique accessions in Papilloma_codes.csv\n",
      "  …processed 12/12 (~0.70 acc/s overall)\n",
      "[STATS] Papilloma: 12 accessions in 17.1s ≈ 0.70 acc/s\n",
      "Writing 103 CDS entries to FASTA\n",
      "[OK] Wrote /Users/ishaharris/Projects/ribolings/data/virus/dna_cds_fasta/Papilloma.fasta with 103 records.\n",
      "[OK] All headers contained [gbkey=CDS].\n",
      "[OK] All sequences A/C/G/T/N only.\n",
      "[OK] No failed accessions.\n",
      "\n",
      "=== Parvo ===\n",
      "Found 9 unique accessions in Parvo_codes.csv\n",
      "  …processed 9/9 (~0.65 acc/s overall)\n",
      "[STATS] Parvo: 9 accessions in 13.9s ≈ 0.65 acc/s\n",
      "Writing 39 CDS entries to FASTA\n",
      "[OK] Wrote /Users/ishaharris/Projects/ribolings/data/virus/dna_cds_fasta/Parvo.fasta with 39 records.\n",
      "[OK] All headers contained [gbkey=CDS].\n",
      "[OK] All sequences A/C/G/T/N only.\n",
      "[OK] No failed accessions.\n",
      "\n",
      "=== Polyomar ===\n",
      "Found 9 unique accessions in Polyomar_codes.csv\n",
      "  …processed 9/9 (~0.72 acc/s overall)\n",
      "[STATS] Polyomar: 9 accessions in 12.4s ≈ 0.72 acc/s\n",
      "Writing 57 CDS entries to FASTA\n",
      "[OK] Wrote /Users/ishaharris/Projects/ribolings/data/virus/dna_cds_fasta/Polyomar.fasta with 57 records.\n",
      "[OK] All headers contained [gbkey=CDS].\n",
      "[OK] All sequences A/C/G/T/N only.\n",
      "[OK] No failed accessions.\n",
      "\n",
      "=== Pox ===\n",
      "Found 11 unique accessions in Pox_codes.csv\n",
      "  …processed 11/11 (~0.54 acc/s overall)\n",
      "[STATS] Pox: 11 accessions in 20.5s ≈ 0.54 acc/s\n",
      "Writing 1946 CDS entries to FASTA\n",
      "[OK] Wrote /Users/ishaharris/Projects/ribolings/data/virus/dna_cds_fasta/Pox.fasta with 1946 records.\n",
      "[OK] All headers contained [gbkey=CDS].\n",
      "[WARN] 3 sequences contain chars outside A/C/G/T/N. Logged to /Users/ishaharris/Projects/ribolings/data/virus/dna_cds_fasta/Pox_nonACGTN_sequences.txt\n",
      "[OK] No failed accessions.\n"
     ]
    }
   ],
   "source": [
    "# === Retrieve GenBank CDS FASTAs by accession (robust parsing + checks + tester) ===\n",
    "from __future__ import annotations\n",
    "\n",
    "import time, re\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from Bio import Entrez\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "# --------------- CONFIG (edit these) ---------------\n",
    "ENTREZ_EMAIL   = \"inah2@cam.ac.uk\"\n",
    "ENTREZ_API_KEY = None  # or paste your key string, e.g. \"fe0657db7c3e1518b265167f026e11288a07\"\n",
    "\n",
    "# Input CSVs like \"Sedereo_codes.csv\" with a column of accessions (e.g. \"Codes\")\n",
    "INPUT_DIR  = Path(\"/Users/ishaharris/Projects/ribolings/data/virus/codes/dna\")\n",
    "\n",
    "# Output folder for combined FASTAs + logs\n",
    "OUTPUT_DIR = Path(\"/Users/ishaharris/Projects/ribolings/data/virus/dna_cds_fasta/\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Families to process (None = auto-discover all *_codes.csv in INPUT_DIR)\n",
    "VIRUS_FAMILIES: Optional[List[str]] = None\n",
    "\n",
    "# Throttle between requests\n",
    "SLEEP = 0.2  # ~3 req/s without key; ~0.1–0.2 with key\n",
    "\n",
    "# Progress + skip rules\n",
    "PROGRESS_EVERY = 25\n",
    "MIN_RECORDS_TO_SKIP = 1\n",
    "# ------------- /CONFIG ----------------\n",
    "\n",
    "# Apply Entrez credentials + validate API key\n",
    "Entrez.email = ENTREZ_EMAIL\n",
    "_key = (ENTREZ_API_KEY or \"\")\n",
    "_key = _key.strip() if isinstance(_key, str) else \"\"\n",
    "if _key and re.fullmatch(r\"[A-Za-z0-9\\-]{20,80}\", _key):\n",
    "    Entrez.api_key = _key\n",
    "    print(f\"[INFO] Using NCBI API key (len={len(_key)}).\")\n",
    "else:\n",
    "    Entrez.api_key = None\n",
    "    if _key:\n",
    "        print(f\"[WARN] API key looks invalid (len={len(_key)}). Not using it.\")\n",
    "    else:\n",
    "        print(\"[INFO] No API key set; using default quota (~3 req/s).\")\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def fasta_record_count(path: Path) -> int:\n",
    "    \"\"\"Count FASTA records (lines starting with '>').\"\"\"\n",
    "    try:\n",
    "        if not path.exists() or path.stat().st_size == 0:\n",
    "            return 0\n",
    "        return sum(1 for line in path.open() if line.startswith(\">\"))\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def entrez_search_nuccore(accession: str) -> List[str]:\n",
    "    \"\"\"Search nuccore by accession; return a list of IDs (may be empty).\"\"\"\n",
    "    query = f\"{accession}[accession]\"\n",
    "    with Entrez.esearch(db=\"nuccore\", term=query) as handle:  # XML default\n",
    "        data = Entrez.read(handle)\n",
    "    return data.get(\"IdList\", []) or []\n",
    "\n",
    "def efetch_cds(nuccore_id: str) -> Optional[str]:\n",
    "    \"\"\"Fetch CDS nucleotide FASTA text for a single nuccore id; return None if not available.\"\"\"\n",
    "    try:\n",
    "        with Entrez.efetch(db=\"nuccore\", id=nuccore_id, rettype=\"fasta_cds_na\", retmode=\"text\") as h:\n",
    "            return h.read()\n",
    "    except HTTPError as e:\n",
    "        # 400 typically means no CDS view for this record (e.g., wrong type / no CDS features)\n",
    "        if e.code == 400:\n",
    "            return None\n",
    "        raise\n",
    "\n",
    "def split_fasta_blob(blob: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split a multi-FASTA blob into individual records, preserving exactly one leading '>' per record.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    current = []\n",
    "    for line in blob.splitlines():\n",
    "        if line.startswith(\">\"):\n",
    "            if current:\n",
    "                records.append(\"\\n\".join(current))\n",
    "            current = [line]  # start new record with header line\n",
    "        else:\n",
    "            current.append(line)\n",
    "    if current:\n",
    "        records.append(\"\\n\".join(current))\n",
    "    return [r for r in records if r.strip()]\n",
    "\n",
    "def retrieve_cds_records(accession: str) -> Optional[List[str]]:\n",
    "    \"\"\"\n",
    "    Search by accession; fetch rettype=fasta_cds_na for the first hit.\n",
    "    Return a list of raw FASTA records (each starting with '>'), or None if none.\n",
    "    \"\"\"\n",
    "    ids = entrez_search_nuccore(accession)\n",
    "    time.sleep(SLEEP)\n",
    "    if not ids:\n",
    "        return None\n",
    "\n",
    "    blob = efetch_cds(ids[0])\n",
    "    time.sleep(SLEEP)\n",
    "    if not blob:\n",
    "        return None\n",
    "\n",
    "    return split_fasta_blob(blob) or None\n",
    "\n",
    "def parse_fasta_records(fasta_records: List[str]) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Robust FASTA parsing:\n",
    "      - header = first line after '>'\n",
    "      - sequence = concat of remaining lines with whitespace removed\n",
    "    Returns (headers, sequences).\n",
    "    \"\"\"\n",
    "    headers, seqs = [], []\n",
    "    for rec in fasta_records:\n",
    "        rec = rec.lstrip()\n",
    "        if not rec.startswith(\">\"):\n",
    "            continue\n",
    "        lines = rec.splitlines()\n",
    "        header = lines[0][1:].strip()  # drop '>'\n",
    "        seq = re.sub(r\"\\s+\", \"\", \"\".join(lines[1:]).upper())\n",
    "        if seq:\n",
    "            headers.append(header)\n",
    "            seqs.append(seq)\n",
    "    return headers, seqs\n",
    "\n",
    "def write_fasta(headers: List[str], seqs: List[str], out_path: Path, line_width: int = 80) -> None:\n",
    "    \"\"\"Write a multi-record FASTA; each header goes after '>'.\"\"\"\n",
    "    assert len(headers) == len(seqs)\n",
    "    def wrap(s: str, w: int) -> List[str]:\n",
    "        return [s[i:i+w] for i in range(0, len(s), w)]\n",
    "    with out_path.open(\"w\") as f:\n",
    "        for h, s in zip(headers, seqs):\n",
    "            f.write(f\">{h}\\n\")\n",
    "            for line in wrap(s, line_width):\n",
    "                f.write(line + \"\\n\")\n",
    "\n",
    "def discover_families(input_dir: Path) -> List[str]:\n",
    "    \"\"\"Auto-discover all families with *_codes.csv files.\"\"\"\n",
    "    fams = []\n",
    "    for p in input_dir.glob(\"*_codes.csv\"):\n",
    "        name = p.stem\n",
    "        if name.endswith(\"_codes\"):\n",
    "            fams.append(name[:-6])\n",
    "    return sorted(set(fams))\n",
    "\n",
    "def load_accessions(csv_path: Path) -> List[str]:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    col = next((c for c in [\"Codes\",\"codes\",\"ACCESSION\",\"accession\",\"Accession\"] if c in df.columns), None)\n",
    "    if col is None:\n",
    "        raise ValueError(f\"No accession column found in {csv_path}. Columns present: {list(df.columns)}\")\n",
    "    accessions = (\n",
    "        df[col]\n",
    "        .dropna()\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .replace(\"\", pd.NA)\n",
    "        .dropna()\n",
    "        .unique()\n",
    "        .tolist()\n",
    "    )\n",
    "    return accessions\n",
    "\n",
    "def process_family(family: str) -> None:\n",
    "    print(f\"\\n=== {family} ===\")\n",
    "    out_fa = OUTPUT_DIR / f\"{family}.fasta\"\n",
    "\n",
    "    # Skip if we already have a non-empty FASTA\n",
    "    n_existing = fasta_record_count(out_fa)\n",
    "    if n_existing >= MIN_RECORDS_TO_SKIP:\n",
    "        print(f\"[SKIP] {family} already processed: {out_fa} ({n_existing} records).\")\n",
    "        return\n",
    "    elif out_fa.exists():\n",
    "        print(f\"[INFO] {out_fa} exists but has {n_existing} records — reprocessing.\")\n",
    "\n",
    "    csv_path = INPUT_DIR / f\"{family}_codes.csv\"\n",
    "    if not csv_path.exists():\n",
    "        print(f\"[WARN] Missing: {csv_path}\")\n",
    "        return\n",
    "\n",
    "    accessions = load_accessions(csv_path)\n",
    "    total = len(accessions)\n",
    "    print(f\"Found {total} unique accessions in {csv_path.name}\")\n",
    "    if total == 0:\n",
    "        print(\"[WARN] No accessions after cleaning; nothing to do.\")\n",
    "        return\n",
    "\n",
    "    # Fetch + parse\n",
    "    all_records: List[str] = []\n",
    "    failed: List[str] = []\n",
    "    t0 = time.time()\n",
    "    processed = 0\n",
    "\n",
    "    for acc in accessions:\n",
    "        try:\n",
    "            recs = retrieve_cds_records(acc)\n",
    "            if recs:\n",
    "                all_records.extend(recs)\n",
    "            else:\n",
    "                failed.append(acc)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERR] {acc}: {e}\")\n",
    "            failed.append(acc)\n",
    "\n",
    "        processed += 1\n",
    "        if (processed % PROGRESS_EVERY == 0) or (processed == total):\n",
    "            rate_total = processed / max(1e-9, (time.time() - t0))\n",
    "            print(f\"  …processed {processed}/{total} (~{rate_total:.2f} acc/s overall)\")\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    if elapsed > 0:\n",
    "        print(f\"[STATS] {family}: {processed} accessions in {elapsed:.1f}s ≈ {processed/elapsed:.2f} acc/s\")\n",
    "\n",
    "    if not all_records:\n",
    "        print(\"[INFO] No CDS records returned for this family.\")\n",
    "        if failed:\n",
    "            fail_path = OUTPUT_DIR / f\"{family}_failed_accessions.txt\"\n",
    "            with fail_path.open(\"w\") as f:\n",
    "                f.write(\"\\n\".join(failed) + \"\\n\")\n",
    "            print(f\"[WARN] {len(failed)} accessions failed. Logged to {fail_path}\")\n",
    "        return\n",
    "\n",
    "    # Parse into headers + sequences\n",
    "    metas, seqs = parse_fasta_records(all_records)\n",
    "    print(f\"Writing {len(seqs)} CDS entries to FASTA\")\n",
    "    write_fasta(metas, seqs, out_fa)\n",
    "    print(f\"[OK] Wrote {out_fa} with {fasta_record_count(out_fa)} records.\")\n",
    "\n",
    "    # ---- Sanity checks ----\n",
    "    nonstandard = [m for m in metas if \"[gbkey=CDS]\" not in m]\n",
    "    if nonstandard:\n",
    "        warn_path = OUTPUT_DIR / f\"{family}_nonstandard_headers.txt\"\n",
    "        with warn_path.open(\"w\") as f:\n",
    "            f.write(\"\\n\".join(nonstandard) + \"\\n\")\n",
    "        print(f\"[WARN] {len(nonstandard)} headers missing [gbkey=CDS]. Logged to {warn_path}\")\n",
    "    else:\n",
    "        print(\"[OK] All headers contained [gbkey=CDS].\")\n",
    "\n",
    "    bad_seq_idx = [i for i, s in enumerate(seqs) if re.search(r\"[^ACGTN]\", s)]\n",
    "    if bad_seq_idx:\n",
    "        bad_path = OUTPUT_DIR / f\"{family}_nonACGTN_sequences.txt\"\n",
    "        with bad_path.open(\"w\") as f:\n",
    "            for i in bad_seq_idx:\n",
    "                f.write(f\">{metas[i]}\\n{seqs[i]}\\n\")\n",
    "        print(f\"[WARN] {len(bad_seq_idx)} sequences contain chars outside A/C/G/T/N. Logged to {bad_path}\")\n",
    "    else:\n",
    "        print(\"[OK] All sequences A/C/G/T/N only.\")\n",
    "\n",
    "    if failed:\n",
    "        fail_path = OUTPUT_DIR / f\"{family}_failed_accessions.txt\"\n",
    "        with fail_path.open(\"w\") as f:\n",
    "            f.write(\"\\n\".join(failed) + \"\\n\")\n",
    "        print(f\"[WARN] {len(failed)} accessions with no CDS returned. Logged to {fail_path}\")\n",
    "    else:\n",
    "        cleanup = OUTPUT_DIR / f\"{family}_failed_accessions.txt\"\n",
    "        if cleanup.exists():\n",
    "            try: cleanup.unlink()\n",
    "            except Exception: pass\n",
    "        print(\"[OK] No failed accessions.\")\n",
    "\n",
    "# -------- Tester: process only first N accessions and write to {family}_TEST{N}.fasta --------\n",
    "def test_family(family: str, n: int = 25) -> None:\n",
    "    print(f\"\\n=== TEST {family} (first {n} accessions) ===\")\n",
    "    csv_path = INPUT_DIR / f\"{family}_codes.csv\"\n",
    "    if not csv_path.exists():\n",
    "        print(f\"[WARN] Missing: {csv_path}\")\n",
    "        return\n",
    "\n",
    "    accessions = load_accessions(csv_path)[:n]\n",
    "    print(f\"Testing with {len(accessions)} accessions from {csv_path.name}\")\n",
    "\n",
    "    all_records: List[str] = []\n",
    "    failed: List[str] = []\n",
    "    t0 = time.time()\n",
    "\n",
    "    for acc in accessions:\n",
    "        try:\n",
    "            recs = retrieve_cds_records(acc)\n",
    "            if recs:\n",
    "                all_records.extend(recs)\n",
    "            else:\n",
    "                failed.append(acc)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERR] {acc}: {e}\")\n",
    "            failed.append(acc)\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"[STATS] TEST {family}: {len(accessions)} accessions in {elapsed:.1f}s\")\n",
    "\n",
    "    if not all_records:\n",
    "        print(\"[INFO] No CDS records returned in test.\")\n",
    "        if failed:\n",
    "            print(f\"[WARN] {len(failed)} failed in test (first few): {failed[:5]}\")\n",
    "        return\n",
    "\n",
    "    metas, seqs = parse_fasta_records(all_records)\n",
    "    out_fa = OUTPUT_DIR / f\"{family}_TEST{n}.fasta\"\n",
    "    write_fasta(metas, seqs, out_fa)\n",
    "    print(f\"[OK] Test wrote {out_fa} with {fasta_record_count(out_fa)} records.\")\n",
    "    # Show a peek at first 2 headers\n",
    "    print(\"Example headers:\")\n",
    "    for h in metas[:2]:\n",
    "        print(\"  >\", h)\n",
    "\n",
    "# ----------------- Run -----------------\n",
    "if VIRUS_FAMILIES is None:\n",
    "    families = discover_families(INPUT_DIR)\n",
    "    print(f\"Discovered {len(families)} families from {INPUT_DIR}:\", families)\n",
    "else:\n",
    "    families = VIRUS_FAMILIES\n",
    "    print(\"Using specified families:\", families)\n",
    "\n",
    "# Example usage:\n",
    "test_family(\"Arena\", n=5)   # quick formatting check\n",
    "for fam in families:           # full run\n",
    "     process_family(fam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec06b0a9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ribolings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
